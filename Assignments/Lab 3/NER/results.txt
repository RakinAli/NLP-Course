FOR WORD EMBEDDINGS:

At termination, Iter: 18175, Sum of Square of Gradient: 0.000005
Model training took 63.49s
Model parameters:
0: -0.4314  1: 0.1813  2: -0.1203  3: -0.8777  4: 0.3047  5: -0.8491  6: -0.7908  7: 0.1034  8: 0.1593  9: -0.0237  10: 0.5758  11: 0.2395  12: -0.3288  13: 0.8659  14: 0.3905  15: 0.9779  16: -0.7116  17: -0.6834  18: 0.6325  19: 0.2190  20: -0.3049  21: -0.7624  22: 0.2583  23: 0.6507  24: 0.5266  25: 0.5144  26: -0.2348  27: -0.1637  28: 0.8080  29: -0.4213  30: -0.6415  31: -0.7564  32: -0.7633  33: -0.0870  34: -0.4766  35: -0.3912  36: 0.6493  37: -1.1636  38: -0.6103  39: 0.4388  40: 0.7940  41: 0.3626  42: -0.5208  43: -0.6371  44: -0.1336  45: -0.1331  46: -0.7502  47: 1.0621  48: 0.1456  49: 0.1332  50: 0.3265  51: 0.4842  52: -0.7682  53: -0.7646  54: -0.1159  55: 0.7329  56: -0.5414  57: 0.0937  58: 1.0321  59: 0.4893  60: 0.2279  61: 0.5272  62: 0.7116  63: -0.9068  64: -0.5714  65: 0.5242  66: -0.4452  67: 0.2433  68: 0.4463  69: 0.1784  70: -0.3124  71: 0.5632  72: -0.4923  73: 0.5079  74: -0.2007  75: 0.3968  76: -0.1219  77: 0.2195  78: -0.5421  79: 0.4972  80: -0.2812  81: -0.9310  82: -0.6959  83: -0.4002  84: 0.5686  85: -0.7512  86: -0.5161  87: -0.0618  88: 0.5715  89: -1.0807  90: 0.2793  91: -0.7622  92: 0.6665  93: -0.2417  94: 0.2062  95: 0.1356  96: 0.0307  97: 0.8033  98: 0.1999  99: -0.6563  100: -0.3990
                       Real class
                        0        1
Predicted class:  0 29353.000  234.000
                  1 55357.000 15054.000

43%

For random indexing:

Model training took 52.43s
Model parameters:
0: -0.7913  1: 0.0811  2: 0.4855  3: -0.2082  4: 0.3611  5: 0.4673  6: -0.9270  7: -0.3086  8: -0.4041  9: -0.7804  10: 0.1434
                       Real class
                        0        1
Predicted class:  0 16652.000  185.000
                  1 68058.000 15103.000